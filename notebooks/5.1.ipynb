{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsngPfkTDSpvbkE8sKMBB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/building-ai-agents/blob/main/notebooks/5.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://ionnova.com/img/ionnova_logo_name_2.png\" width=\"120px\"></td>\n",
        "    <td><h1>Minimal RAG Pipeline with LangChain</h1></td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "nucbTcrfGiNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain==1.1.0 langchain-openai==1.1.0 langchain-community==0.4.1 faiss-cpu==1.13.2 python-dotenv==1.1.1"
      ],
      "metadata": {
        "id": "kCpdB6yJG7jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "\n",
        "# Create the directory\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "\n",
        "# Write some sample content to the file\n",
        "content = \"\"\"\n",
        "LangGraph is a library for building stateful, multi-agent applications with LLMs.\n",
        "It extends the LangChain Expression Language (LCEL) by allowing for cycles and\n",
        "persistence in your agent workflows. Unlike linear chains, LangGraph uses\n",
        "nodes and edges to create complex, iterative processes.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"docs/langgraph_intro.txt\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "\n",
        "# Load and chunk documents\n",
        "loader = TextLoader(\"docs/langgraph_intro.txt\")\n",
        "documents = loader.load()\n",
        "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "# Create embeddings and store in FAISS\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Define the prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Use the context below to answer the question:\\n\\n{context}\\n\\nQ: {question}\"\n",
        ")\n",
        "\n",
        "# Build the RAG pipeline\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "result = rag_chain.invoke(\"What is LangGraph?\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "oT3Gib8eJrG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
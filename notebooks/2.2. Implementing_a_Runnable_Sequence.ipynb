{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/building-ai-agents/blob/main/notebooks/2.2.%20Implementing_a_Runnable_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APhi21Hl_GLU"
      },
      "source": [
        "# Building and Composing Runnables with LangChain Expression Language (LCEL)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/solomontessema/Agentic-AI-with-Python/blob/main/notebooks/Building%20with%20LangChain/Implementing_a_Runnable_Sequence.ipynb)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand what Runnables are and why they replaced legacy Chains\n",
        "2. Master the pipe operator (`|`) for composing processing pipelines\n",
        "3. Learn how to capture and inspect intermediate results\n",
        "4. Build multi-step reasoning workflows using LCEL\n",
        "5. Use `RunnablePassthrough` and `RunnableLambda` for advanced composition\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y2m8fLE_GLV"
      },
      "source": [
        "## Part 1: Understanding Runnables vs. Legacy Chains\n",
        "\n",
        "### What Changed in LangChain?\n",
        "\n",
        "LangChain evolved from using **Class-based Chains** (like `LLMChain`, `SimpleSequentialChain`) to **Runnables** with **LangChain Expression Language (LCEL)**.\n",
        "\n",
        "#### Legacy Approach (Deprecated):\n",
        "```python\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run({\"topic\": \"AI\"})\n",
        "```\n",
        "\n",
        "#### Modern Approach (LCEL):\n",
        "```python\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "result = chain.invoke({\"topic\": \"AI\"})\n",
        "```\n",
        "\n",
        "### Why Runnables?\n",
        "\n",
        "| Feature | Legacy Chains | Runnables (LCEL) |\n",
        "|---------|---------------|------------------|\n",
        "| **Composition** | Verbose class instantiation | Clean pipe operator `\\|` |\n",
        "| **Streaming** | Limited/manual | Built-in `.stream()` |\n",
        "| **Debugging** | Harder to trace | Better introspection |\n",
        "| **Status** | **Deprecated** | **Current Standard** |\n",
        "\n",
        "### Core Runnable Methods\n",
        "\n",
        "Every Runnable implements these methods:\n",
        "\n",
        "- **`invoke(input)`** - Run synchronously and return final result\n",
        "- **`stream(input)`** - Stream outputs as they're generated\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0e9Tsk0_GLW"
      },
      "source": [
        "## Part 2: Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vEQ_08y_GLW"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain-openai langchain-core python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ueIqCuZ4ENL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thg2CRO5_GLW"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API key is set\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHGQB8M3_GLX"
      },
      "source": [
        "## Part 3: Building Your First Runnable\n",
        "\n",
        "### The Simplest Runnable: Prompt â†’ LLM â†’ Parser\n",
        "\n",
        "This is the fundamental pattern in LCEL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzJjYjOo_GLX"
      },
      "outputs": [],
      "source": [
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the latest trends in {topic}?\"\n",
        ")\n",
        "\n",
        "# Compose the chain using the pipe operator\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the chain\n",
        "result = chain.invoke({\"topic\": \"machine learning\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skgvuq3j_GLX"
      },
      "source": [
        "### What Just Happened?\n",
        "\n",
        "1. **`prompt`** - Formats the input dict into a message for the LLM\n",
        "2. **`|`** - The pipe operator passes output to the next step\n",
        "3. **`llm`** - Sends the formatted message to OpenAI's API\n",
        "4. **`StrOutputParser()`** - Extracts the text content from the LLM response\n",
        "\n",
        "The beauty of LCEL is that each component is **composable** - you can rearrange, add, or remove steps easily.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIh5ETi-_GLX"
      },
      "source": [
        "## Part 4: Multi-Step Reasoning Pipelines\n",
        "\n",
        "### Example: Summarize â†’ Rephrase\n",
        "\n",
        "Let's build a two-step pipeline where:\n",
        "1. First step summarizes input text\n",
        "2. Second step rephrases the summary professionally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag3E-CUg_GLX"
      },
      "outputs": [],
      "source": [
        "# Step 1: Summarize input text\n",
        "summarize_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarize this content concisely: {input_text}\"\n",
        ")\n",
        "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Step 2: Rephrase the summary\n",
        "rephrase_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Rephrase this summary in a professional tone: {text}\"\n",
        ")\n",
        "rephrase_chain = rephrase_prompt | llm | StrOutputParser()\n",
        "\n",
        "print(\"âœ… Individual chains created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjYHQmjt_GLX"
      },
      "source": [
        "### Connecting the Chains\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpMmA7l7_GLX"
      },
      "outputs": [],
      "source": [
        "def print_intermediate(x):\n",
        "  print(\"Intermediate Result: \" , x)\n",
        "  return x\n",
        "\n",
        "# Compose pipeline with proper input mapping\n",
        "pipeline_v1 = (\n",
        "    summarize_chain\n",
        "    | RunnableLambda(print_intermediate)\n",
        "    | rephrase_chain\n",
        ")\n",
        "\n",
        "# Test it\n",
        "input_text = \"The global AI market is projected to grow significantly over the next decade.\"\n",
        "result = pipeline_v1.invoke({\"input_text\": input_text})\n",
        "\n",
        "print(\"Final Result:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHt5xMbs_GLY"
      },
      "source": [
        "## Part 5: Advanced Multi-Step Workflow\n",
        "\n",
        "### Use Case: Extract Entities â†’ Summarize â†’ Format\n",
        "\n",
        "Let's build a three-step pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDfV7KKb_GLY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract named entities\n",
        "entity_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Extract all people, organizations, and locations from: {input_text}\\n\"\n",
        "    \"Return as a simple list.\"\n",
        ")\n",
        "entity_chain = entity_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Step 2: Summarize entities\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarize these extracted entities in one sentence: {entities}\"\n",
        ")\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Step 3: Format as a paragraph\n",
        "format_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Format this into a user-friendly paragraph: {summary}\"\n",
        ")\n",
        "format_chain = format_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Compose the full pipeline\n",
        "full_pipeline = (\n",
        "    entity_chain\n",
        "    | summary_chain\n",
        "    | format_chain\n",
        ")\n",
        "\n",
        "# Test it\n",
        "test_input = {\n",
        "    \"input_text\": \"OpenAI and Google DeepMind are competing in developing AGI. \"\n",
        "                  \"Sam Altman and Demis Hassabis are leading their respective organizations.\"\n",
        "}\n",
        "\n",
        "result = full_pipeline.invoke(test_input)\n",
        "print(\"Final Formatted Output:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEWmUXTV_GLY"
      },
      "source": [
        "## Part 6: Parallel Execution with `RunnableParallel`\n",
        "\n",
        "Sometimes you want to run multiple analyses in parallel and combine results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vucgIoH_GLY"
      },
      "outputs": [],
      "source": [
        "# Create parallel analysis chains\n",
        "sentiment_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Analyze the sentiment (positive/negative/neutral) of: {text}\"\n",
        ")\n",
        "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
        "\n",
        "keywords_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Extract 3-5 key keywords from: {text}\"\n",
        ")\n",
        "keywords_chain = keywords_prompt | llm | StrOutputParser()\n",
        "\n",
        "language_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Classify the language style (formal/informal/technical) of: {text}\"\n",
        ")\n",
        "language_chain = language_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run all three in parallel\n",
        "parallel_chain = RunnableParallel(\n",
        "    sentiment=sentiment_chain,\n",
        "    keywords=keywords_chain,\n",
        "    language=language_chain\n",
        ")\n",
        "\n",
        "# Test it\n",
        "analysis_result = parallel_chain.invoke({\n",
        "    \"text\": \"The quarterly earnings exceeded expectations, demonstrating robust growth \"\n",
        "            \"in our core business segments.\"\n",
        "})\n",
        "\n",
        "print(\"Parallel Analysis Results:\\n\")\n",
        "print(f\"Sentiment: {analysis_result['sentiment']}\")\n",
        "print(f\"Keywords: {analysis_result['keywords']}\")\n",
        "print(f\"Language Style: {analysis_result['language']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcrhNWsQ_GLY"
      },
      "source": [
        "### ðŸš€ Performance Note\n",
        "\n",
        "When using `RunnableParallel`, all chains execute **concurrently**. This means:\n",
        "- Faster execution (all API calls happen at once)\n",
        "- More efficient use of resources\n",
        "- Better for independent analyses\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jVTvOIZ_GLY"
      },
      "source": [
        "## Part 7: Streaming Results\n",
        "\n",
        "One major advantage of LCEL is built-in streaming support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UpdZufm_GLZ"
      },
      "outputs": [],
      "source": [
        "# Create a chain for streaming\n",
        "streaming_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short story about {topic} in 3 paragraphs.\"\n",
        ")\n",
        "streaming_chain = streaming_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Stream the output\n",
        "print(\"Streaming story about AI...\\n\")\n",
        "for chunk in streaming_chain.stream({\"topic\": \"a friendly AI robot\"}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\nâœ… Streaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7vg0Ti__GLZ"
      },
      "source": [
        "### Why Streaming Matters\n",
        "\n",
        "- **Better UX**: Users see results immediately instead of waiting\n",
        "- **Faster perceived performance**: First token latency vs. full completion\n",
        "- **Long-form content**: Essential for essays, reports, stories\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}